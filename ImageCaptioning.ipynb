{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGBW8vlAsQRO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -u timm\n",
        "!pip install -u albumentations\n",
        "!pip install -U torchtext\n",
        "!pip install --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 torchtext==0.18.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import albumentations as alb\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Hyperparameters\n",
        "class ImageCaptioner(nn.Module):\n",
        "  def __init__(self, context_lenght, vocabulary_size, num_blocks, model_dim, num_heads, prob):\n",
        "    super().__init__()\n",
        "    self.cnn_encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "    test_image = torch.zeros(1, 3, 224, 224)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      cnn_output = self.cnn_encoder(test_image)\n",
        "    in_features = cnn_output.shape[1]\n",
        "    self.project = nn.Linear(in_features, model_dim)\n",
        "\n",
        "    self.word_embeddings = nn.Embedding(vocabulary_size, model_dim)\n",
        "    self.position_embeddings = nn.Embedding(context_lenght, model_dim)\n",
        "\n",
        "    block = nn.TransformerDecoderLayer(model_dim, num_heads, 2 * model_dim, dropout=prob, batch_first=True, norm_first=True)\n",
        "    self.blocks = nn.TransformerDecoder(block, num_blocks)\n",
        "\n",
        "    self.vocab_projection = nn.Linear(model_dim, vocabulary_size)\n",
        "\n",
        "\n",
        "  def forward(self, image, true_labels):\n",
        "    token_embedded = self.word_embeddings(true_labels)\n",
        "    B, T = true_labels.shape\n",
        "    positions = torch.arange(T).to(device)\n",
        "    position_embedded = self.position_embeddings(positions)\n",
        "    total_embeddings = token_embedded + position_embedded\n",
        "\n",
        "    with torch.no_grad():\n",
        "      encoded_image = self.project(self.cnn_encoder(image).view(B, -1))\n",
        "\n",
        "    img_for_attention = torch.unsqueeze(encoded_image, 1)\n",
        "\n",
        "    attention_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n",
        "    block_output = self.blocks(total_embeddings, img_for_attention, tgt_mask=attention_mask)\n",
        "\n",
        "    vocabulary_vector = self.vocab_projection(block_output)\n",
        "\n",
        "    return vocabulary_vector"
      ],
      "metadata": {
        "id": "rBRWLAfRsdYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/flick8r\"\n",
        "\n",
        "caption_filename = 'captions.txt'\n",
        "missing = \"2258277193_586949ec62.jpg\"\n",
        "\n",
        "with open(caption_filename) as captions:\n",
        "  lines = captions.readlines()\n",
        "\n",
        "get_captions = {}\n",
        "all_captions = [] #5 * len(get_captions)\n",
        "\n",
        "for caption in lines:\n",
        "  data = caption.rstrip('\\n').split('.jpg,')\n",
        "  img_name = data[0] + '.jpg'\n",
        "  if img_name == missing:\n",
        "    continue\n",
        "\n",
        "  caption_list = get_captions.get(img_name, [])\n",
        "  caption_list.append(data[1])\n",
        "  get_captions[img_name] = caption_list\n",
        "  all_captions.append(data[1])\n",
        "\n",
        "df = pd.DataFrame(columns=['filename', 'caption'])\n",
        "df['filename'] = get_captions.keys()\n",
        "df['caption'] = df['filename'].map(lambda filename: get_captions[filename])\n",
        "\n",
        "vocab_frequency = Counter()\n",
        "word_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "for cap in all_captions:\n",
        "  vocab_frequency.update(word_tokenizer(cap))\n",
        "\n",
        "vocabulary = torchtext.vocab.vocab(vocab_frequency)\n",
        "vocabulary.insert_token('<UNKNOWN>', 0)\n",
        "vocabulary.insert_token('<PAD>', 1)\n",
        "vocabulary.insert_token('<START>', 2)\n",
        "vocabulary.insert_token('<END>', 3)\n",
        "vocabulary.set_default_index(0)\n",
        "\n",
        "context_lenght = 20\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "  def __init__(self, split):\n",
        "    self.df = df\n",
        "    self.img_size = 224\n",
        "    transformation_list = [alb.Resize(self.img_size, self.img_size)]\n",
        "    if split == 'training':\n",
        "      transformation_list.append(alb.HorizontalFlip())\n",
        "      transformation_list.append(alb.ColorJitter())\n",
        "    transformation_list.append(alb.Normalize())\n",
        "    transformation_list.append(ToTensorV2())\n",
        "\n",
        "    self.transformations = alb.Compose(transformation_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_filename, captions = self.df.iloc[idx]\n",
        "    actual_image = cv2.cvtColor(cv2.imread('Images/' + image_filename), cv2.COLOR_BGR2RGB)\n",
        "    cv2_imshow(actual_image)\n",
        "    transformed_img = self.transformations(image=actual_image)['image']\n",
        "\n",
        "    encoded_captions = [] # Initialize the list here\n",
        "    for i, cap in enumerate(captions):\n",
        "      splitted = word_tokenizer(cap)\n",
        "\n",
        "      integers = [vocabulary[word] for word in splitted]\n",
        "      integers = [2] + integers + [3]\n",
        "\n",
        "      if len(integers) <= context_lenght:\n",
        "        pads_to_add = context_lenght - len(integers)\n",
        "        integers += [1] * pads_to_add\n",
        "      else:\n",
        "        integers = integers[:context_lenght - 1] + [3]\n",
        "\n",
        "      encoded_captions.append(torch.tensor(integers, dtype=torch.long))\n",
        "\n",
        "    random_idx = torch.randint(5, (1,)).item()\n",
        "    return transformed_img, encoded_captions[random_idx]\n",
        "\n",
        "training_dataset = ImageCaptioningDataset('training')\n",
        "training_data = DataLoader(training_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "MMeL5aTvsg1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_lenght = 20\n",
        "vocabulary_size = len(vocabulary)\n",
        "num_blocks = 6\n",
        "model_dim = 512\n",
        "num_heads = 16\n",
        "prob = 0.5\n",
        "\n",
        "model = ImageCaptioner(context_lenght, vocabulary_size, num_blocks, model_dim, num_heads, prob).to(device)\n",
        "\n",
        "for layer in model.cnn_encoder.parameters():\n",
        "  layer.requires_grad = False\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=vocabulary['<PAD>'])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "VEfH41bI1mR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "num_iterations = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for images, captions in training_data:\n",
        "    images, captions = images.to(device), captions.to(device)\n",
        "    B, T = captions.shape\n",
        "    model_prediction = model(images, captions)\n",
        "    model_prediction = model_prediction.view(B * T, vocabulary_size)\n",
        "    loss = loss_function(model_prediction, captions.view(B*T))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "    optimizer.step()\n",
        "    if num_iterations % 100 == 0:\n",
        "      print(loss.item())\n",
        "    num_iterations += 1"
      ],
      "metadata": {
        "id": "cu0TEDu3_3BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integer_to_word = vocabulary.get_itos()\n",
        "num_epochs = 1\n",
        "num_iterations = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x, y in training_data:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    prediction = model(x, y)\n",
        "    _, indices = torch.max(prediction, dim = -1)\n",
        "    first_caption = indices[0]\n",
        "    sentence = []\n",
        "    for id in first_caption:\n",
        "      sentence.append(integer_to_word[id])\n",
        "      if id == 3:\n",
        "        break\n",
        "    print(' '.join(sentence))\n",
        "    B, T, vocabulary_size = prediction.shape\n",
        "    prediction = prediction.view(B * T, vocabulary_size)\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_function(prediction, y.view(B*T))\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "    num_iterations += 1"
      ],
      "metadata": {
        "id": "wgzrRHC1B1vv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}